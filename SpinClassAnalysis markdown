---
runtime: shiny
title: Spin Class Analysis
author: Jessica Chew | 4 August 2021 | [GitHub](https://github.com/jessicacychew/Spin-class-performance-analysis)
output:
  html_document:
    theme: lumen
    toc: true
    toc_float: true
---

<style>
.list-group-item.active, .list-group-item.active:focus, .list-group-item.active:hover {
    background-color: #28b62c;
}
</style>

![](rukusstudio.png)
<center>Image from Rukus Cycling Studios: https://gocyclenow.com/
</center>

# Introduction & Motivation
I have been attending a spin studio that uses smart bikes and after each session I receive an e-mail summarising my key metrics such as miles travelled, calories burned, average RPMs and class rank amongst other things.

I was curious as to my performance trends over time and given that I can't access the studio's app for reporting, I decided to scrape the data from each individual e-mail into a dataset for analysis and visualisation.

This analysis has three parts:

1. A tutorial on extracting and refining data from Gmail using a mix of Google Apps scripts and SQL and/or Python;
2. A Tableau spin class dashboard, AKA Spinboard;
3. Further spin data analysis exploring key relationships that drive spin class performance.

# 1. Data extraction


We are going to extract the data from our Gmail accounts with Google Apps Scripts. But first, what is that?

* A JavaScript based scripting language;
* Built into Google apps e.g., Google Sheets, Google Docs, Gmail etc;
* Written in Google Chrome's own script editor;
* Automates tasks in Google apps;
* Runs on Google's servers.

In short, it's JavaScript in a native Google environment that is very nifty!

Here is an example of the original spin class' performance summary e-mail.


<img src="rukus_screen_shot.png" style='border:1px solid #D3D3D3'/>


Our objective is to extract, transform and load the key metrics from each of these spin class e-mails into a dataframe ready for analysis.

Target metrics:

* Calories
* Distance
* Points
* Avg RPM
* Max RPM
* Avg Watts
* Max Watts

The overall process is:

1. Use Gmail App Scripts to extract the data from Gmail into a Google sheet;
2. Pipe the Google sheet data into a SQL or other scripting environment for efficient manipulation;
3. Pre-process the raw e-mail data and parse out relevant metrics to create the final dataset ready for analysis.

For steps 2 & 3 I will provide both a BigQuery and Python option.

The raw data pulled straight out of Google sheets is also available in my [GitHub repo](https://github.com/jessicacychew/Spin-class-performance-analysis) under 'rukus_results_raw.csv'.

## Step 1 - Gmail App Scripts Gmail Extraction

Here we will extract the data out of Gmail into a Google Sheet.

First and foremost credit must go to Code Wondo and his extremely helpful YouTube tutorial [here](https://www.youtube.com/watch?v=gdgCVqtcIw4&ab_channel=CodeWondo). As he has already done an excellent job in his tutorial, I highly recommend watching his video for the broad overview of how to open a fresh Google sheet, access the scripting environment, grant access permissions and get started with key syntax.

Here is what my raw ride results data look like freshly pulled from my Gmail to Google sheets. (Note I hard coded the column headers.)

<img src="rukus_into_googlesheet.png" style='border:1px solid #D3D3D3'/>

Please find below the code I used to extract the date, sender, subject and body content from each individual spin class e-mail.

### Apps script code

```
///Google Apps Script begin///
function onOpen(e) {
  var ui = SpreadsheetApp.getUi();
  ui.createMenu("Jessica's Menu").addItem("Get e-mails", "getGmailEmails").addToUi();
}

function getGmailEmails(){
  var label = GmailApp.getUserLabelByName("Rideresults");
  var threads = label.getThreads();

  for(var i = threads.length - 1; i >=0; i-- ) {
    var messages = threads[i].getMessages();

    for (var j = 0; j <messages.length; j++){
      var message = messages[j];
      extractDetails (message);

    }
  }
}
function extractDetails(message){
  var dateTime = message.getDate();
  var subjectText = message.getSubject();
  var senderDetails = message.getFrom();
  var bodyContents =  message.getPlainBody();

  var activeSheet = SpreadsheetApp.getActiveSpreadsheet().getActiveSheet();
  activeSheet.appendRow([dateTime, senderDetails, subjectText, bodyContents])
}
///Script end///
```
## Step 2a (BigQuery): Piping in data to a scripting environment

Fab! You have now pulled your e-mail data into a spreadsheet at scale! But of course any data we encounter in the wild is going to be messy. There are many ways to manipulate and clean up dataframes from Excel, Google sheets, Python and R amongst others. SQL is my default weapon of choice. (That's what you get for cutting your analytics teeth as a straight up data analyst - but I digress.)

I have a [free tier account](https://cloud.google.com/free/?gclid=Cj0KCQjw_8mHBhClARIsABfFgpjrah5ClIsBnwJh5sCZNK70IXVwezo7fA8LPfvk65ttnkKLixF4-ioaArX9EALw_wcB&gclsrc=aw.ds) with Google Cloud Platform which allows me access to BigQuery (done are my days wrangling MySQL instances and its clunky interface).

Since I only have the free sandbox version of BQ, I cannot stream in the Google sheets data using commands. Instead I will use the BQ console to manually create a table of raw spin class results. You can also use whatever version of SQL you have access to. You can pipe the data in per your familiarity and rejoin us at step 3.

BQ manual load steps:

1. Create a project in your BQ space. I called my project 'rukus'.

<img src="pin_project.png" style='border:1px solid #D3D3D3'/>

2. Go back to your Google sheet and copy the sharing link by clicking on the top right green 'Share' button. The sheet does not need to be made public.

3. On BQ in the Query Editor of your new project, click the '+' button to create a new table.

<img src="create_table_button.png" style='border:1px solid #D3D3D3'/>

4. Follow the prompts below to ingest the Google sheet data into a table in BQ.

<img src="create_table.png" style='border:1px solid #D3D3D3'/>

5. Great, we've now piped in the Google sheets data into BQ - ready for speedier code-based data transformation.

<img src="raw_data_bq.png" style='border:1px solid #D3D3D3'/>

## Step 3a (BigQuery): Data pre-processing

The exciting part - we are now ready to cleanse and parse out the relevant spin class metrics into a squeaky clean dataset! Here's what we're aiming for.

<img src="clean_result_head.png" style='border:1px solid #D3D3D3'/>

### Query
Please find detailed comments on each element of the query below to achieve the above output.

```
#### Query starts
### QUERY PART 1 OF 3 - Pipe in Google sheets data and transform the body of the e-mail into shorter, relevant strings for parsing at later stage.
WITH results_stage AS
(
SELECT cast(date as date) ride_date, REGEXP_EXTRACT(REPLACE(REPLACE(REPLACE(body, '\r', ''), '\n', ''), " ",""), r"^.{200}") body_clean
## Above replace and regex statements remove the carriage lines from the raw data, eliminates all blank spaces to concatenate all characters and finally keeps only the first 200 characters as they carry relevant data.
FROM `boxwood-airport-304120.rukus.rukus_results_raw`
WHERE 1=1
AND Date is not null
AND LOWER(Body) NOT LIKE '%impulse%' -- we are only interested in the Rukus Prime rides
)

### QUERY PART 2 OF 3 - Parsing out relevant data from soupy strings to create output dataset

, results_clean AS

(SELECT
ride_date
,body_clean
,REGEXP_EXTRACT(REGEXP_EXTRACT(body_clean, r"(?i)...CALORIES"), r"\d+") calories
##Nested REGEX: First gather the three characters before the string 'calories' plus the string itself (expressed by the three full stops), and then only return digits of those results. Key assumption: Calories burned do not exceed three digits.

,REGEXP_EXTRACT(REGEXP_EXTRACT(body_clean, r"(?i)...POINTS"), r"\d+") points
##Nested REGEX: First gather the three characters before the string 'points' plus the string itself (expressed by the three full stops), and then only return digits of those results (expressed by r"\d+"). Key assumption: Points achieved do not exceed three digits.

,REGEXP_EXTRACT(REGEXP_EXTRACT(body_clean, r"(?i)......DISTANCE"), r"\d+.+\d") distance_mi
##Nested REGEX: First gather the six characters before the string 'distance' plus the string itself (expressed by the three full stops). The result is e.g., '10.2miDistance'. Then only return digits and decimal points of those results. Key assumption: Miles ridden do not exceed 99.9.

,REGEXP_EXTRACT(REGEXP_EXTRACT(REGEXP_EXTRACT(body_clean, r"(?i)......avg/maxrpm") , r"^.{3}") , r"\d+") avg_rpm
##Nested REGEX: First gather the six characters before the string 'avg/maxrpm' plus the string itself (expressed by the three full stops). The result is e.g., '71/134AVG/MAXRPM'. Then only return the first three characters (symbolised by "r"&.{3}") of those results. Finally, of those results, return only the numbers (removing slashes etc). Key assumption: Avg RPMs do not exceed three digits.

,REGEXP_EXTRACT(REGEXP_EXTRACT(REGEXP_EXTRACT(body_clean, r"(?i)......avg/maxrpm") , r"(?i)...AVG") , r"\d+") max_rpm
##Nested REGEX: First gather the six characters before the string 'avg/maxrpm' plus the string itself (expressed by the three full stops). The result is e.g., '71/134AVG/MAXRPM'. Then only return the three characters before the string 'AVG'. Finally, of those results, return only the digits (removing slashes/random characters etc). Key assumption: Max RPMs do not exceed three digits.

,REGEXP_EXTRACT(REGEXP_EXTRACT(REGEXP_EXTRACT(body_clean, r"(?i).......avg/maxwatts") , r"^.{3}") , r"\d+") avg_watts
##Same as Avg RPM approach

,REGEXP_EXTRACT(REGEXP_EXTRACT(REGEXP_EXTRACT(body_clean, r"(?i)......avg/maxwatts") , r"(?i)...AVG") , r"\d+") max_watts
##Same as Max RPM approach

,CASE WHEN REGEXP_EXTRACT(REGEXP_EXTRACT(body_clean, r"(?i)..RANK"), r"\d+") IS NULL THEN REGEXP_EXTRACT(REGEXP_EXTRACT(body_clean, r"(?i)..CLASSRANK"), r"\d+")
ELSE REGEXP_EXTRACT(REGEXP_EXTRACT(body_clean, r"(?i)..RANK"), r"\d+") END AS rank
## Conditional nested REGEX statements: The spin studio changed their e-mail data structure after 28 March 2021. The verbiage went from "5RANK" to denote 5th rank in class to "3ClassRank". Statement prepares REGEX patterns for both types of verbiage and leverages conditional logic to surface one or the other in case of a NULL result. Key assumption: Rank does not exceed two digits.

FROM results_stage)

### QUERY PART 3 OF 3 - Remove a duplicate entry from 22 November 2020. Spin class had a systems glitch that day. The glitch entry includes small value metrics such as a calorie burn of 7.

select * except (body_clean)
from results_clean
where 1=1
and concat(ride_date, calories) <> '2020-11-227'

### End query

```



Let's compare our original July 4th e-mail example to the parsed results.

_Original e-mail_
<img src="rukus_screen_shot.png" style='border:1px solid #D3D3D3'/>


<br>
_Parsed results_
<img src="clean_result.png" style='border:1px solid #D3D3D3'/>

Looking good! You can also check out the Python approach below. Otherwise, onto your next step of analysis!


## Step 2 & 3 (Python): Data pre-processing


Fab! You have now pulled your e-mail data into a spreadsheet at scale! But of course any data we encounter in the wild is going to be messy.

We can use Python to conduct our Google sheets data transformation and produce the below cleansed output.

<img src="python_rukus_output.png" style='border:1px solid #D3D3D3'/>

### Python script
Below are the annotated steps to transforming the raw Google sheets data into a clean and useable output dataset in Python.

```
### Python script starts

import pandas as pd
import numpy as np
import datetime
import re
import base64  
from IPython.display import HTML

## Pipe in raw Google sheets data from your computer. I saved a CSV version of the Google sheets data.
#df = pd.read_csv('/Users/jessicachew/Downloads/rukus_results_raw.csv')

## Convert Date from a datetime format to a date format.
df['ride_date'] = pd.to_datetime(df['Date']).dt.date

## Reduce the dataframe to the two main data points of interest
df = df.filter(items=['ride_date', 'Body'])

## Filter out any entries related to 'Impulse' classes.
## I am focusing in 'Prime' ride classes which are the majority that I took.
df = df[~df['Body'].str.contains('Impulse', na=False)]

## Remove line breaks from the raw body data part 1 of 2
df = df.replace(to_replace=r'\n',value='', regex=True)

## Remove line breaks from the raw body data part 2 of 2
df = df.replace(to_replace=r'\r',value='', regex=True)

## Remove all empty spces from the the data. This will help standardise the parsing later on.
df = df.replace(to_replace=r' ',value='', regex=True)

###### Reduce body text to first 200 characters
search = []    
for values in df['Body']:
    search.append(re.search(r'^.{200}', values).group())

df['body_clean'] = search

###### Extract calories data 1 of 2
## First gather the three characters before the string 'calories' plus the string itself (expressed by the three full stops)
search = []    
for values in df['body_clean']:
    search.append(re.search(r'(?i)...CALORIES', values).group())

df['calories'] = search

###### Extract calories data 2 of 2
## Only return digits of the results. Key assumption: Calories burned do not exceed three digits.

search = []    
for values in df['calories']:
    search.append(re.search(r'\d+', values).group())

df['calories'] = search

###### Extract points data 1 of 2
## Same principle as above
search = []    
for values in df['body_clean']:
    search.append(re.search(r'(?i)...POINTS', values).group())

df['points'] = search

###### Extract points data 2 of 2
## Same principle as above
search = []    
for values in df['points']:
    search.append(re.search(r'\d+', values).group())

df['points'] = search

###### Extract distance data 1 of 2
## First gather the six characters before the string 'distance' plus the string itself (expressed by the three full stops). The result is e.g., '10.2miDistance'
search = []    
for values in df['body_clean']:
    search.append(re.search(r'(?i)......DISTANCE', values).group())

df['distance_mi'] = search

###### Extract distance data 2 of 2
## Only return digits and decimal points of those results. Key assumption: Miles ridden do not exceed 99.9.
search = []    
for values in df['distance_mi']:
    search.append(re.search(r'\d+.+\d', values).group())

df['distance_mi'] = search

###### Extract avg_rpm data 1 of 3
## First gather the six characters before the string 'avg/maxrpm' plus the string itself (expressed by the three full stops). The result is e.g., '71/134AVG/MAXRPM'
search = []    
for values in df['body_clean']:
    search.append(re.search(r'(?i)......avg/maxrpm', values).group())

df['avg_rpm'] = search

###### Extract avg_rpm data 2 of 3
## Only return the first three characters (symbolised by "r'&.{3}'') of those results.
search = []    
for values in df['avg_rpm']:
    search.append(re.search(r'^.{3}', values).group())

df['avg_rpm'] = search

###### Extract avg_rpm data 3 of 3
## Return only the numbers (removing slashes etc). Key assumption: Avg RPMs do not exceed three digits.
search = []    
for values in df['avg_rpm']:
    search.append(re.search(r'\d+', values).group())

df['avg_rpm'] = search

###### Extract max_rpm data 1 of 3
## Same principle for step 1 of 3 in the avg_rpm section
search = []    
for values in df['body_clean']:
    search.append(re.search(r'(?i)......avg/maxrpm', values).group())

df['max_rpm'] = search

###### Extract max_rpm data 2 of 3
## Return the three characters before the string 'AVG'
search = []    
for values in df['max_rpm']:
    search.append(re.search(r'(?i)...AVG', values).group())

df['max_rpm'] = search

###### Extract max_rpm data 3 of 3
## Same principle for step 3 of 3 in the avg_rpm section
search = []    
for values in df['max_rpm']:
    search.append(re.search(r'\d+', values).group())

df['max_rpm'] = search

###### Extract avg_watts data 1 of 3
## Same principle for avg_rpm section
search = []    
for values in df['body_clean']:
    search.append(re.search(r'(?i).......avg/maxwatts', values).group())

df['avg_watts'] = search

###### Extract avg_watts data 2 of 3
## Same principle for avg_rpm section
search = []    
for values in df['avg_watts']:
    search.append(re.search(r'^.{3}', values).group())

df['avg_watts'] = search

###### Extract avg_watts data 3 of 3
## Same principle for avg_rpm section
search = []    
for values in df['avg_watts']:
    search.append(re.search(r'\d+', values).group())

df['avg_watts'] = search

###### Extract max_watts data 1 of 3
## Same principle for max_rpm section
search = []    
for values in df['body_clean']:
    search.append(re.search(r'(?i)......avg/maxwatts', values).group())

df['max_watts'] = search

###### Extract max_watts data 2 of 3
## Same principle for max_rpm section
search = []    
for values in df['max_watts']:
    search.append(re.search(r'(?i)...AVG', values).group())

df['max_watts'] = search

###### Extract max_watts data 3 of 3
## Same principle for max_rpm section
search = []    
for values in df['max_watts']:
    search.append(re.search(r'\d+', values).group())

df['max_watts'] = search

## Temporarily halving the dataframe.
## The spin studio changed their e-mail data structure after 28 March 2021.
## The verbiage went from "5RANK" to denote 5th rank in class to "3ClassRank".
## Create two temporary dataframes, one for each type of verbiage to apply custom regex for each scenario.

## Create temp df 1
pre_change_start = pd.to_datetime("2020-1-01").date() ##arbitrary start date - before I started spin class
pre_change_end = pd.to_datetime("2021-3-28").date()

after_start_date = df["ride_date"] > pre_change_start
before_end_date = df["ride_date"] <= pre_change_end
between_two_dates = after_start_date & before_end_date
df_temp_1 = df.loc[between_two_dates]

## Create temp df 2
post_change_start = pd.to_datetime("2021-3-28").date()
post_change_end = pd.to_datetime("2022-3-28").date() ##arbitrary future end date

after_start_date = df["ride_date"] > post_change_start
before_end_date = df["ride_date"] <= post_change_end
between_two_dates = after_start_date & before_end_date
df_temp_2 = df.loc[between_two_dates]

###### Extract df_temp_1 rank 1 of 2
search = []    
for values in df_temp_1['body_clean']:
    search.append(re.search(r'(?i)..RANK', values).group())

df_temp_1['rank1'] = search

###### Extract df_temp_1 rank 2 of 2
search = []    
for values in df_temp_1['rank1']:
    search.append(re.search(r'\d+', values).group())

df_temp_1['rank1'] = search

###### Extract df_temp_2 rank 1 of 2
search = []    
for values in df_temp_2['body_clean']:
    search.append(re.search(r'(?i)..CLASSRANK', values).group())

df_temp_2['rank2'] = search

###### Extract df_temp_2 rank 2 of 2
search = []    
for values in df_temp_2['rank2']:
    search.append(re.search(r'\d+', values).group())

df_temp_2['rank2'] = search

###### Union the two dataframes back together. For entries up until 28 March 2021, rank 1 will have entries whilst rank 2 will show null. After this date, the inverse will occur.
frames = [df_temp_1, df_temp_2]
df = pd.concat(frames)

##### Create a final clean rank that surfaces a rank number where either rank 1 or rank 2 is null.
df['rank'] = np.select(
[
    (df['rank1'].isnull())
],
[
    df['rank2']
],

default=df['rank1'])

##### Remove a duplicate entry from 22 November 2020. Spin class had a systems glitch that day.
##### The glitch entry includes small value metrics such as a calorie burn of 7.
##### Create a unique fingerprint by concatenating the below metrics
df['tempconcat'] = df['calories'] + df['points'] + df['distance_mi'] + df['avg_rpm']

##### The concatenated calories, points, distance and avg_rpm of the offending entry is '72550.485'. Let's exclude it.
df = df[~df['tempconcat'].str.contains('72550.485', na=False)]

##### Penultimate step - let's select the final columns necessary for our output table.
df = df.filter(items=['ride_date', 'calories', 'points','distance_mi','avg_rpm','max_rpm','avg_watts','max_watts','rank'])
df

# ##Export download link for summary for analysis

def create_download_link(df, title = "Download CSV file", filename = "df.csv"):  
    csv = df.to_csv(index =False)
    b64 = base64.b64encode(csv.encode())
    payload = b64.decode()
    html = '<a download="{filename}" href="data:text/csv;base64,{payload}" target="_blank">{title}</a>'
    html = html.format(payload=payload,title=title,filename=filename)
    return HTML(html)
create_download_link(df)

### End script
```
# 2. Tableau Spinboard
```{r, echo = FALSE, out.width=920}
knitr::include_url("https://public.tableau.com/views/Spinboard/Spinboard?:showVizHome=no&:embed=true", height=1300)
```


# 3. Exploratory analysis
## 3.1. Target optmisation metric
This section extends the above Tableau reporting further and examines the relationships between the extracted spin class data points.

In particular, we are interested in what drives average watts. Watts, or power, is the gold standard metric to objectively measure a cyclist's performance. Within a cycling context it is defined as:

```
Power = All resisting forces (i.e., wind, gravity, spin bike resistance setting) x Velocity
```

This is a nifty sports metric that is independent of factors like temperature, fatigue, altitude, caffeine intake etc. that influence other fitness metrics such as heart rate. Watts allow the cyclist to:

* Measure all the forces acting against them;
* Compare wattage of a given ride to wattage on any other ride;
* Compare efforts to any other rider in the world (after converting into power-to-weight ratio per the Spinboard reporting).

One thing to note before we get started: the spin class e-mails do not include the average resistance setting which is a critical element of determining watts.  The instructor will recommend a resistance setting depending on the sequence. For example, for a flat road ride, the resistance will be lower compared to a heavy hill climb.

'Resistance' will vary depending on the different spin studios and different models of bike. At best I can supply that of a Rukus bike that goes up to about 20 notches of resistance, I estimate that I typically ride at a resistance level between 7 - 16, with an average of 13.

So, given all of that, can the spin class dataset generate insights into how I can drive my watts performance?

## 3.2. Scatterplot
Let's load up some R libraries.

```{r, results = 'hide' , message=FALSE,setup}
library(tidyverse)
library(ggplot2)
library(Hmisc)
library(RColorBrewer)
library(corrplot)
library(SPARQL)
library(reshape2)
library(corrplot)
source("http://www.sthda.com/upload/rquery_cormat.r")
library(Hmisc)
library(purrr)
library(rmdformats)
library(webshot)
```

Reading in our spin class dataset generated from data extraction step 1.


```{r, comment=NA}
df <- read.csv('rukus_ride_results_clean.csv')
tibble(df)
```

Then, we want to understand the relationship between all the different metrics. I really like the pairplot visualisation from the Python Seaborn package for its ease of use and impactful visualisation. (It is possible to run Python in RStudio via the ```reticulate``` package but I have some operating system limitations so I ran the pairplot separately in Python.)

<img src="seaborn_pairplot.png" style='border:1px solid #D3D3D3'/>

We observe an inverse linear relationship between average RPM and calories and average RPM and points (scatterplots 1 and 2) and more of an interesting relationship between average RPM and watts (3). There appears to be no relationship between rank and all the other metrics. This is expected as my class rank will be relative to how strong the other riders in the class are in a given session. Put another way; if I were in a spin class with the last 10 Tour de France winners, it doesn't matter how fast I pedal, I will rank 11th!

## 3.3. Correlation matrix

Let's see the pearson correlation matrix between these metrics at a 95% confidence level.

```{r , comment = NA}
# Read data and remove the ride date (not needed for the computations)
mtx <- read.csv('rukus_ride_results_clean.csv') %>% select (-ride_date)

# Compute the correlation matrix
corrmtx.rcorr = rcorr(as.matrix(mtx))

# Extract out the correlation matrix
mydata.coeff = corrmtx.rcorr$r

# Extract out the p-value matrix of the correlations
mydata.p = corrmtx.rcorr$P

# Set a colour palette
col1 <- colorRampPalette(brewer.pal(9,"RdYlGn"))

#Produce the matrix
try(corrplot(mydata.coeff , method = "square", order = "FPC", diag = FALSE,title="Spin Class Correlation Matrix" ,tl.col = "black", tl.cex = 0.75, p.mat = mydata.p, sig.level = 0.05, insig = "pch", pch.cex = 1 , col = col1(100), mar = c(1,1,1,1)))

```


The larger the squares and the brighter the colours denote a stronger positive or negative correlation whilst squares with crosses show where the correlation is not significant (i.e., the p-value is greater than 0.05).

Some intuitive results do jump out at us. Calories burned are very strongly correlated with (Rukus) points earned and distance ridden. Average watts are also strongly positively correlated with calorie burn and points earned whilst being inversely correlated with average RPM.

## 3.4. The next spin class challenge

In closing out this exploratory assessment we know from the Spinboard that my average watts performance in the last year has improved in tandem with an average RPM slowdown. What's not captured but implied in the dataset is my increase in bike resistance over time which we observe generates more power and points overall, but drags down my speed.

The key takeaway and fitness challenge is to *pick up my speed up from its yearlong average RPM of `r ceiling(mean(df[["avg_rpm"]]))` but not at the cost of my resistance level.* I'll know that I've progressed my riding abilities when higher average RPMs become associated with higher power scores. More simply: ride heavy, but ride _fast_. 

